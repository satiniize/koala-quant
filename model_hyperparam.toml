[model]
vocab_size = 1000   # Add this line
input_dim = 1
d_model = 64
num_heads = 8
num_layers = 4
d_ff = 256
max_seq_length = 30
dropout = 0.1

[training]
batch_size = 64
num_epochs = 200
learning_rate = 0.0005
lr_scheduler_step = 10
lr_scheduler_gamma = 0.95

[data]
training_tickers = [
	{ ticker = "BBRI.JK", period = "5y" },
	{ ticker = "BMRI.JK", period = "5y" },
	# { ticker = "TLKM.JK", period = "2y" },
	{ ticker = "BBNI.JK", period = "5y" },
	# { ticker = "UNVR.JK", period = "2y" },
	# { ticker = "INDF.JK", period = "2y" },
	# { ticker = "MYOR.JK", period = "2y" },
	# { ticker = "PGAS.JK", period = "2y" },
	# { ticker = "JSMR.JK", period = "2y" },
]
sequence_length = 30
train_test_split = 0.8
features = ["Open", "High", "Low", "Close", "Volume"]

[attention]
hidden_size = 64
num_heads = 1
dropout = 0.2

[prediction]
ticker_symbol = "BMRI.JK"
future_days = 64
