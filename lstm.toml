[model]
vocab_size = 1000    # Add this line
input_dim = 1
d_model = 128
num_heads = 8
num_layers = 4
d_ff = 256
max_seq_length = 128
dropout = 0.1

[training]
batch_size = 64
num_epochs = 1000
learning_rate = 0.001
lr_scheduler_step = 10
lr_scheduler_gamma = 0.95
criterion = "CrossEntropyLoss"
optimizer = "Adam"
save_path = "koala-quant.pth"
validation_split = 0.2
checkpoint_frequency = 10
early_stopping_patience = 10
early_stopping_metric = "val_loss"

[checkpointing]
enable_checkpoints = true
checkpoint_dir = "checkpoints"
checkpoint_frequency = 10
keep_last_n = 5

[data]
period = "5y"
training_tickers = [
	"BBRI.JK",
	"BMRI.JK",
	"TLKM.JK",
	"BBNI.JK",
	"UNVR.JK",
	"INDF.JK",
	"PGAS.JK",
	"JSMR.JK",
	"ASII.JK",
	"ICBP.JK",
	"ANTM.JK",
	"PTBA.JK",
	"HMSP.JK",
	"KLBF.JK",
	"SMGR.JK",
	"ADRO.JK",
]
sequence_length = 30
train_test_split = 0.8
features = "Close"

[attention]
hidden_size = 64
num_heads = 1
dropout = 0.2

[prediction]
ticker_symbol = "BMRI.JK"
future_days = 64
